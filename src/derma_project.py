# -*- coding: utf-8 -*-
"""derma_project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s3iMc5DqytZE8EY65PnEFVCPxW9ohmT8
"""

import os
import shutil
import numpy as np
import pandas as pd
from PIL import Image


import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.cuda.amp import autocast, GradScaler
from torchvision import transforms


from sklearn.model_selection import train_test_split
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import f1_score

import timm

# ==============================
# âš™ï¸ Environment
# ==============================
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
os.environ["CUDA_LAUNCH_BLOCKING"] = "1"


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

from google.colab import files
files.upload()  # select kaggle.json from your computer

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# HAM10000
!kaggle datasets download -d kmader/skin-cancer-mnist-ham10000 -p /content/skin_datasets/HAM10000
!unzip -q /content/skin_datasets/HAM10000/skin-cancer-mnist-ham10000.zip -d /content/skin_datasets/HAM10000

#DermNetNZ subset
!kaggle datasets download -d shubhamgoel27/dermnet -p /content/skin_datasets/DermNetNZ
!unzip -q /content/skin_datasets/DermNetNZ/dermnet.zip -d /content/skin_datasets/DermNetNZ

# ===========================
# ğŸ“‚ ISIC 2019 Dataset Setup (KaggleHub)
# ===========================

import os, shutil
import kagglehub

# Download via KaggleHub
path = kagglehub.dataset_download("andrewmvd/isic-2019")
print("ğŸ“¥ Downloaded to:", path)

# Base dir for project
base_dir = "/content/skin_datasets/ISIC2019"
os.makedirs(base_dir, exist_ok=True)

# Copy dataset contents to base_dir
for item in os.listdir(path):
    s = os.path.join(path, item)
    d = os.path.join(base_dir, item)
    if os.path.isdir(s):
        shutil.copytree(s, d, dirs_exist_ok=True)
    else:
        shutil.copy2(s, d)

print("âœ… ISIC 2019 dataset ready at:", base_dir)

# ===========================
# ğŸ“‘ Load labels CSV
# ===========================
import pandas as pd

csv_path = os.path.join(base_dir, "ISIC_2019_Training_GroundTruth.csv")
df = pd.read_csv(csv_path)

# Attach full file paths
image_dir = os.path.join(base_dir, "ISIC_2019_Training_Input")
df["image_path"] = df["image"].apply(lambda x: os.path.join(image_dir, f"{x}.jpg"))

print("âœ… DataFrame ready with", len(df), "samples")
df.head()

import os
import pandas as pd

# ===========================
# ğŸ“ Define dataset paths
# ===========================
ham_paths = [
    "/content/skin_datasets/HAM10000/HAM10000_images_part_1",
    "/content/skin_datasets/HAM10000/HAM10000_images_part_2",
    "/content/skin_datasets/HAM10000/ham10000_images_part_1",
    "/content/skin_datasets/HAM10000/ham10000_images_part_2",
]
ham_meta_path = "/content/skin_datasets/HAM10000/HAM10000_metadata.csv"

isic_image_dir = "/content/skin_datasets/ISIC2019/ISIC_2019_Training_Input/ISIC_2019_Training_Input"
isic_csv_path = "/content/skin_datasets/ISIC2019/ISIC_2019_Training_GroundTruth.csv"

dermnet_dir = "/content/skin_datasets/DermNetNZ/train"

records = []

# ===========================
# ğŸ©º HAM10000
# ===========================
if os.path.exists(ham_meta_path):
    ham_meta = pd.read_csv(ham_meta_path)
    ham_labels = dict(zip(ham_meta["image_id"], ham_meta["dx"]))

    ham_images = []
    for folder in ham_paths:
        if os.path.exists(folder):
            for f in os.listdir(folder):
                if f.lower().endswith(".jpg"):
                    img_id = os.path.splitext(f)[0]
                    label = ham_labels.get(img_id, "unknown")
                    if label != "unknown":
                        ham_images.append({
                            "image_path": os.path.join(folder, f),
                            "label": label
                        })
    df_ham = pd.DataFrame(ham_images)
    print(f"âœ… HAM10000 loaded successfully with {len(df_ham)} valid images")
else:
    df_ham = pd.DataFrame(columns=["image_path", "label"])
    print("âš ï¸ HAM10000 metadata not found!")

# ===========================
# ğŸ§¬ ISIC2019
# ===========================
if os.path.exists(isic_csv_path):
    df_isic = pd.read_csv(isic_csv_path)
    df_isic["label"] = df_isic.drop(columns=["image"]).idxmax(axis=1)
    df_isic["image_path"] = df_isic["image"].apply(
        lambda x: os.path.join(isic_image_dir, f"{x}.jpg")
    )
    df_isic = df_isic[df_isic["image_path"].apply(os.path.exists)]
    df_isic = df_isic[["image_path", "label"]]
    print(f"âœ… ISIC2019 loaded successfully with {len(df_isic)} valid images")
else:
    df_isic = pd.DataFrame(columns=["image_path", "label"])
    print("âš ï¸ ISIC2019 CSV not found!")

# ===========================
# ğŸŒ¿ DermNetNZ
# ===========================
derm_records = []
if os.path.exists(dermnet_dir):
    for root, dirs, files in os.walk(dermnet_dir):
        label = os.path.basename(root)
        for f in files:
            if f.lower().endswith((".jpg", ".jpeg", ".png")):
                derm_records.append({
                    "image_path": os.path.join(root, f),
                    "label": label
                })
    df_derm = pd.DataFrame(derm_records)
    print(f"âœ… DermNetNZ loaded successfully with {len(df_derm)} valid images")
else:
    df_derm = pd.DataFrame(columns=["image_path", "label"])
    print("âš ï¸ DermNetNZ directory not found!")

# ===========================
# ğŸ§© Combine all datasets
# ===========================
df_all = pd.concat([df_ham, df_isic, df_derm], ignore_index=True)
df_all = df_all[df_all["image_path"].apply(os.path.exists)]
df_all = df_all[df_all["label"] != "unknown"].reset_index(drop=True)

# ===========================
# ğŸ“Š Summary
# ===========================
print("\nâœ… Final unified dataset summary:")
print("Total images:", len(df_all))
print("Unique disease classes:", df_all['label'].nunique())
print("\nTop 10 classes:")
print(df_all['label'].value_counts().head(10))

df_all.head()

# ===========================
# ğŸ§¬ Normalize only duplicate shortcodes, preserve all unique classes
# ===========================

# Mapping only for the known abbreviations â€” everything else stays untouched
label_map = {
    "akiec": "Actinic Keratosis / Intraepithelial Carcinoma",
    "bcc": "Basal Cell Carcinoma",
    "bkl": "Benign Keratosis",
    "df": "Dermatofibroma",
    "nv": "Melanocytic Nevus",
    "mel": "Melanoma",
    "vasc": "Vascular Lesion",
    "sek": "Seborrheic Keratosis",
}

# Normalize case
df_all["label"] = df_all["label"].str.strip().str.lower()

# Apply mapping only where relevant; keep others untouched
df_all["label"] = df_all["label"].apply(lambda x: label_map.get(x, x))

# Drop unknown/empty labels
df_all = df_all[df_all["label"].notna() & (df_all["label"] != "unknown")].reset_index(drop=True)

# âœ… Verify
print("âœ… Labels normalized â€” only duplicates merged, all unique ones preserved")
print("Total images:", len(df_all))
print("Unique disease classes:", df_all['label'].nunique())
print("\nTop 20 classes after normalization:")
print(df_all['label'].value_counts().head(20))

# ==============================
# ğŸ“‚ Dataset
# ==============================
train_df, val_df = train_test_split(
    df_all,
    test_size=0.15,
    stratify=df_all['label'],
    random_state=42
)


print("Train samples:", len(train_df))
print("Validation samples:", len(val_df))

# ==============================
# âš–ï¸ Class Weights
# ==============================
class_weights = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(train_df['label']),
    y=train_df['label']
)
class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)


criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.1)

# ==============================
# ğŸ¨ Transforms
# ==============================
train_transforms = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomVerticalFlip(),
    transforms.RandomRotation(15),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])


val_transforms = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# ==============================
# ğŸ“¦ Dataset class
# ==============================
class SkinDataset(Dataset):
    def __init__(self, df, transform=None):
        self.df = df.reset_index(drop=True)
        self.transform = transform
        self.classes = sorted(df['label'].unique())
        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}

    def __len__(self):
        return len(self.df)   # âœ… Must return dataset size

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        image = Image.open(row['image_path']).convert('RGB')
        label = self.class_to_idx[row['label']]
        if self.transform:
            image = self.transform(image)
        return image, label


train_dataset = SkinDataset(train_df, transform=train_transforms)
val_dataset = SkinDataset(val_df, transform=val_transforms)


train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2, pin_memory=True)
val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=2, pin_memory=True)


print("âœ… DataLoaders ready!")

# ==============================
# ğŸ§  Model
# ==============================
num_classes = len(train_dataset.classes)


model = timm.create_model(
    'convnextv2_base.fcmae_ft_in22k_in1k',
    pretrained=True,
    num_classes=num_classes
).to(device)


optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)
scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=2, T_mult=2)
scaler = GradScaler()

# ==============================
# ğŸ”’ Progressive Unfreezing
# ==============================
def freeze_all_except_head():
  for param in model.parameters():
    param.requires_grad = False
  for param in model.head.parameters():
    param.requires_grad = True


def unfreeze_last_stage():
  for param in model.stages[-1].parameters():
    param.requires_grad = True


def unfreeze_last_two_stages():
  for param in model.stages[-2].parameters():
    param.requires_grad = True


# Initially freeze backbone
freeze_all_except_head()

# ==============================
# ğŸ’¾ Checkpoint Handling
# ==============================
ckpt_dir = "checkpoints"
os.makedirs(ckpt_dir, exist_ok=True)


start_epoch, best_acc = 0, 0.0


if os.path.exists(ckpt_dir):
  checkpoints = [f for f in os.listdir(ckpt_dir) if f.endswith(".pth")]
  if checkpoints:
    latest_ckpt = os.path.join(ckpt_dir, sorted(checkpoints)[-1])
    print(f"ğŸ”„ Resuming from checkpoint: {latest_ckpt}")
    state = torch.load(latest_ckpt, map_location=device)
    model.load_state_dict(state['model'])
    optimizer.load_state_dict(state['optimizer'])
    scheduler.load_state_dict(state['scheduler'])
    scaler.load_state_dict(state['scaler'])
    start_epoch = state['epoch'] + 1
    best_acc = state['best_acc']

# ==============================
# ğŸ” Training Loop
# ==============================
epochs = 12

print("ğŸš€ Starting training on", device)

for epoch in range(start_epoch, epochs):
    # Progressive unfreezing
    if epoch == 2:
        unfreeze_last_stage()
        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
            optimizer, T_0=2, T_mult=2
        )
    if epoch == 4:
        unfreeze_last_two_stages()
        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
            optimizer, T_0=2, T_mult=2
        )

    model.train()
    train_loss, correct, total = 0, 0, 0

    for imgs, labels in train_loader:
        imgs, labels = imgs.to(device, non_blocking=True), labels.to(device, non_blocking=True)
        optimizer.zero_grad()

        with autocast():
            outputs = model(imgs)
            loss = criterion(outputs, labels)

        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
        scheduler.step(epoch + total/len(train_loader))

        train_loss += loss.item() * imgs.size(0)
        _, preds = outputs.max(1)
        total += labels.size(0)
        correct += preds.eq(labels).sum().item()

    train_acc = 100 * correct / total

    # Validation
    model.eval()
    val_loss, val_correct, val_total = 0, 0, 0
    all_preds, all_labels = [], []

    with torch.no_grad():
        for imgs, labels in val_loader:
            imgs, labels = imgs.to(device, non_blocking=True), labels.to(device, non_blocking=True)
            with autocast():
                outputs = model(imgs)
                loss = criterion(outputs, labels)

            val_loss += loss.item() * imgs.size(0)
            _, preds = outputs.max(1)
            val_total += labels.size(0)
            val_correct += preds.eq(labels).sum().item()

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    val_acc = 100 * val_correct / val_total
    val_f1 = f1_score(all_labels, all_preds, average="macro")

    print(f"ğŸ“… Epoch [{epoch+1}/{epochs}] | "
          f"Train Acc: {train_acc:.2f}% | "
          f"Val Acc: {val_acc:.2f}% | "
          f"Val F1: {val_f1:.4f}")

    # Save checkpoint
    ckpt_path = os.path.join(ckpt_dir, f"model_epoch_{epoch+1}.pth")
    torch.save({
        'epoch': epoch,
        'model': model.state_dict(),
        'optimizer': optimizer.state_dict(),
        'scheduler': scheduler.state_dict(),
        'scaler': scaler.state_dict(),
        'best_acc': best_acc
    }, ckpt_path)

    if val_acc > best_acc:
        best_acc = val_acc
        torch.save(model.state_dict(), "best_model.pth")
        print(f"ğŸ† New best model saved with {best_acc:.2f}% validation accuracy!")

print(f"ğŸ¯ Training complete. Best validation accuracy: {best_acc:.2f}%")